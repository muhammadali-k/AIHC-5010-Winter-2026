{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# Assignment 1 — Colab Workflow (GitHub + Pre-commit + Submission Validation)\n",
    "\n",
    "This notebook teaches the standard workflow used throughout the course:\n",
    "\n",
    "1. Clone your team repo\n",
    "2. Install dependencies\n",
    "3. Install **pre-commit** and enable a hook to strip notebook outputs\n",
    "4. Run this notebook end-to-end\n",
    "5. Validate `predictions.csv`\n",
    "6. Commit + push + tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1",
    "outputId": "1ece67ba-d47d-4420-87f4-15e2f0203e4f"
   },
   "outputs": [],
   "source": [
    "# (Colab) show python and system info\n",
    "import sys, platform\n",
    "print(sys.version)\n",
    "print(platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "source": [
    "## 1) Clone Repo\n",
    "\n",
    "Login to your personal Github account, and make a fork of: https://github.com/TLKline/AIHC-5010-Winter-2026\n",
    "\n",
    "Follow setup directions for working with a PAT in GitHub (30-second guide):\n",
    "\n",
    "* Go to GitHub → Settings\n",
    "* Developer settings\n",
    "* Personal access tokens\n",
    "* Choose:\n",
    "  * Fine-Grained\n",
    "\n",
    "You can clone using HTTPS.\n",
    "\n",
    "Repo HTTPS URL (e.g., `https://github.com/TLKline/AIHC-5010-Winter-2026.git`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3",
    "outputId": "c8a50146-2f6a-4416-dc80-a19d87622ad0"
   },
   "outputs": [],
   "source": [
    "# TODO: Change the following to your github repo path\n",
    "repo_path = 'https://github.com/TLKline/AIHC-5010-Winter-2026.git'\n",
    "!git clone {repo_path} student_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4",
    "outputId": "1925eced-707e-4ab9-a9fd-e4e853a3c5d9"
   },
   "outputs": [],
   "source": [
    "# Move into repo\n",
    "%cd student_repo\n",
    "\n",
    "# Repo git info\n",
    "!git status\n",
    "\n",
    "# Where are we?\n",
    "print('----------')\n",
    "print('We are at:')\n",
    "!pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "5"
   },
   "source": [
    "## 2) Install dependencies\n",
    "\n",
    "This installs whatever is in `requirements.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6",
    "outputId": "155e7c46-d392-498c-c533-d09b030e7c95"
   },
   "outputs": [],
   "source": [
    "!pip -q install -r Project-1/readmit30/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "7"
   },
   "source": [
    "## 3) Enable pre-commit hook to strip notebook outputs\n",
    "\n",
    "This prevents giant notebooks and reduces merge/diff pain.\n",
    "\n",
    "One-time per clone:\n",
    "- `pre-commit install`\n",
    "\n",
    "After that, every `git commit` will strip outputs from `*.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8",
    "outputId": "41ab2a36-b268-4dfe-8611-7ed2d2a67e89"
   },
   "outputs": [],
   "source": [
    "!pip -q install pre-commit\n",
    "!pre-commit install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "99fd509c"
   },
   "source": [
    "#MAINSTART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "14"
   },
   "source": [
    "# 4) Submission Notebook (Template)\n",
    "\n",
    "Replace the baseline model with your team’s approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15",
    "outputId": "18a2230b-a90a-4387-f60e-ecd9881691aa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "TRAIN_PATH = os.environ.get(\"TRAIN_PATH\", \"Project-1/readmit30/scripts/data/public/train.csv\")\n",
    "DEV_PATH   = os.environ.get(\"DEV_PATH\",   \"Project-1/readmit30/scripts/data/public/dev.csv\")\n",
    "TEST_PATH  = os.environ.get(\"TEST_PATH\",  \"Project-1/readmit30/scripts/data/public/public_test.csv\")\n",
    "OUT_PATH   = os.environ.get(\"OUT_PATH\",   \"predictions.csv\")\n",
    "\n",
    "print(\"TRAIN_PATH:\", TRAIN_PATH)\n",
    "print(\"DEV_PATH:\", DEV_PATH)\n",
    "print(\"TEST_PATH:\", TEST_PATH)\n",
    "print(\"OUT_PATH:\", OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "16"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "assert \"row_id\" in train.columns and \"readmit30\" in train.columns\n",
    "assert \"row_id\" in test.columns\n",
    "\n",
    "X_train = train.drop(columns=[\"readmit30\"])\n",
    "y_train = train[\"readmit30\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "17",
    "outputId": "930b4eeb-8599-4c46-e343-82cf700c8a75"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "# TODO: Add any new imports for your own method here\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "method = 1\n",
    "\n",
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols),\n",
    "    ],\n",
    ")\n",
    "\n",
    "if method==1:\n",
    "    # Use logistic regression model\n",
    "    clf = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", LogisticRegression(max_iter=200)),\n",
    "    ])\n",
    "\n",
    "if method==2:\n",
    "    # Use logistic regression model\n",
    "    clf = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", LogisticRegression(max_iter=200,class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "if method==3:\n",
    "    # Use SVC (i.e. SVM model)\n",
    "    clf = Pipeline(\n",
    "        [\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"scaler\", StandardScaler(with_mean=False)), # Add StandardScaler here\n",
    "            (\"model\", SVC(gamma=\"auto\",max_iter=1000,probability=True)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "if method == 4:\n",
    "    # Preprocess for HGB: ordinal-encode categories (HGB needs numeric inputs)\n",
    "    preprocess_hgb = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            ]), num_cols),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "            ]), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    clf = Pipeline([\n",
    "        (\"preprocess\", preprocess_hgb),\n",
    "        (\"model\", HistGradientBoostingClassifier(\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            max_iter=300,\n",
    "            l2_regularization=1.0,\n",
    "            early_stopping=True,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "18",
    "outputId": "dd2d086f-9167-4b75-da99-859f2a34d334"
   },
   "outputs": [],
   "source": [
    "p_test = clf.predict_proba(test)[:, 1]\n",
    "pred = pd.DataFrame({\"row_id\": test[\"row_id\"].astype(int), \"prob_readmit30\": p_test.astype(float)})\n",
    "pred.to_csv(OUT_PATH, index=False)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19",
    "outputId": "dc432d2e-0dc7-4268-9807-c3b62a954cca"
   },
   "outputs": [],
   "source": [
    "# Validate output format (required for students before tagging)\n",
    "!python Project-1/readmit30/scripts/validate_submission.py --pred {OUT_PATH} --test {TEST_PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for the dev set\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dev = pd.read_csv(DEV_PATH)\n",
    "\n",
    "X_dev = dev.drop(columns=[\"readmit30\"])\n",
    "y_dev = dev[\"readmit30\"].astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = y_dev.astype(int)\n",
    "y_pred = clf.predict_proba(X_dev)[:, 1]\n",
    "\n",
    "auroc = roc_auc_score(y_true, y_pred)\n",
    "auprc = average_precision_score(y_true, y_pred)\n",
    "brier = brier_score_loss(y_true, y_pred)\n",
    "\n",
    "print(f'AUROC: {auroc:.4f}')\n",
    "print(f'AUPRC: {auprc:.4f}')\n",
    "print(f'Brier Score: {brier:.4f}')\n",
    "\n",
    "# Create figures\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histogram of predicted probabilities\n",
    "plt.hist(y_pred, bins=20, alpha=0.7, label='Predicted Probabilities')\n",
    "plt.title('Histogram of Predicted Probabilities')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of true vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.5, label='True vs Predicted')\n",
    "plt.title('True vs Predicted Probabilities')\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel('Predicted Probabilities')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUROC = {auroc:.4f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create Precision-Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'AUPRC = {auprc:.4f}')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create Confusion Matrix Heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "threshold = 0.5  # Default threshold for binary classification\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Readmit', 'Readmit'], yticklabels=['No Readmit', 'Readmit'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "d42abe98"
   },
   "source": [
    "#MAINEND"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "20"
   },
   "source": [
    "## 5) Validate the predictions file format\n",
    "\n",
    "This checks:\n",
    "- required columns\n",
    "- probabilities in [0, 1]\n",
    "- row_ids match the test file\n",
    "\n",
    "It assumes the submission notebook wrote `predictions.csv` in the repo root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21",
    "outputId": "42773310-af89-4b10-ceb2-a61b8e384982"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "pred_path = Path(\"predictions.csv\")\n",
    "test_path = Path(\"Project-1/readmit30/scripts/data/public/public_test.csv\")\n",
    "\n",
    "if not pred_path.exists():\n",
    "    print(\"predictions.csv not found. Run notebooks/submission.ipynb first.\")\n",
    "else:\n",
    "    !python Project-1/readmit30/scripts/validate_submission.py --pred predictions.csv --test Project-1/readmit30/scripts/data/public/public_test.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "22"
   },
   "source": [
    "## 6) Commit + push + tag\n",
    "\n",
    "You will:\n",
    "- add changes\n",
    "- commit (pre-commit hook runs here)\n",
    "- push\n",
    "- tag a milestone (example: `milestone_wk3`) and push tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "xQucvfTCla8Z"
   },
   "source": [
    "You will need a Personal Access Token (PAT) for the following step. See instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23",
    "outputId": "e5c269fc-0942-46d5-cf6c-efe316f18336"
   },
   "outputs": [],
   "source": [
    "import getpass, subprocess\n",
    "\n",
    "# TODO: Update with your Github credentials\n",
    "# ========= USER-EDITABLE SETTINGS (change only these) =========\n",
    "GIT_AUTHOR_NAME   = \"Timothy Kline\"             # shows in commits\n",
    "GIT_AUTHOR_EMAIL  = \"kline.timothy@gmail.com\"   # shows in commits\n",
    "GITHUB_USERNAME   = \"tlkline\"                   # GitHub login for auth\n",
    "# =============================================================\n",
    "\n",
    "# Identity\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_AUTHOR_NAME], check=True)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_AUTHOR_EMAIL], check=True)\n",
    "\n",
    "# Use the plain \"store\" helper (persists for the *runtime*, not your local machine)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=True)\n",
    "\n",
    "token = getpass.getpass(\"GitHub PAT: \").strip()\n",
    "\n",
    "# Approve credentials for github.com\n",
    "cred_input = (\n",
    "    \"protocol=https\\n\"\n",
    "    \"host=github.com\\n\"\n",
    "    f\"username={GITHUB_USERNAME}\\n\"\n",
    "    f\"password={token}\\n\\n\"\n",
    ")\n",
    "subprocess.run([\"git\", \"credential\", \"approve\"], input=cred_input.encode(), check=True)\n",
    "\n",
    "# Quick auth test (doesn't modify anything)\n",
    "subprocess.run([\"git\", \"ls-remote\", \"origin\", \"-h\"], check=True)\n",
    "\n",
    "print(\"Auth looks good. Now you can: git push\")\n",
    "\n",
    "# Commit and push\n",
    "!pre-commit run --all-files\n",
    "!git add -A\n",
    "# Run one more time to force the changes and push\n",
    "!pre-commit run --all-files\n",
    "!git add -A\n",
    "!git commit -m \"Assignment 1: workflow + initial submission notebook\"\n",
    "!git push\n",
    "\n",
    "TAG = \"checking_workflow_007\"\n",
    "!git tag -a {TAG} -m \"Checking workflow 007\"\n",
    "!git push --tags\n",
    "print(\"Tagged and pushed:\", TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "26"
   },
   "source": [
    "## Done ✅\n",
    "\n",
    "If you hit issues:\n",
    "- Make sure you pulled the latest course template (missing files).\n",
    "- Make sure `data/public/*` exists in your repo (or your instructor provided it separately).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
